{
  
    
        "post0": {
            "title": "Object Detection with Icevision",
            "content": "What is Icevision ? . Icevision is a framework agnostic object detection library that enables us to train our models with just a few lines using fastai or pytorch lightning.Icevision makes training mmdetection,torchvision and efficientdet models extremely easy.All we need to do is change a couple lines.Let&#39;s get started. . Installation . We start by installing Icevision.Installing Icevision is pretty easy,all we need to do is run the cell below.Beware that we are running on colab using gpu.If you are on your local machine and need to use cpu,change the below line from cuda to cpu.Icevision only supports Linux and MAC at the moment(via installation below) but you can install its dependencies manually to run it on Windows.We also install torch,torchvision,mmdetection,yolov5 and efficientdet in the cell below. . # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # Choose your installation target: cuda11 or cuda10 or cpu !bash icevision_install.sh cuda11 master . --2022-08-07 15:54:31-- https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2820 (2.8K) [text/plain] Saving to: ‘icevision_install.sh’ icevision_install.s 100%[===================&gt;] 2.75K --.-KB/s in 0s 2022-08-07 15:54:31 (26.6 MB/s) - ‘icevision_install.sh’ saved [2820/2820] Installing icevision + dependencices for cuda11 - Installing torch and its dependencies Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Looking in links: https://download.pytorch.org/whl/torch_stable.html Collecting torch==1.10.0+cu111 Downloading https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2137.6 MB) |████████████▌ | 834.1 MB 1.5 MB/s eta 0:14:30tcmalloc: large alloc 1147494400 bytes == 0x3a39c000 @ 0x7f3db06e5615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7 |███████████████▉ | 1055.7 MB 1.4 MB/s eta 0:12:46tcmalloc: large alloc 1434370048 bytes == 0x7e9f2000 @ 0x7f3db06e5615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7 |████████████████████ | 1336.2 MB 53.1 MB/s eta 0:00:16tcmalloc: large alloc 1792966656 bytes == 0x3824000 @ 0x7f3db06e5615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7 |█████████████████████████▎ | 1691.1 MB 1.4 MB/s eta 0:05:25tcmalloc: large alloc 2241208320 bytes == 0x6e60c000 @ 0x7f3db06e5615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7 |████████████████████████████████| 2137.6 MB 1.4 MB/s eta 0:00:01tcmalloc: large alloc 2137645056 bytes == 0xf3f6e000 @ 0x7f3db06e41e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 tcmalloc: large alloc 2672058368 bytes == 0x1e7ac4000 @ 0x7f3db06e5615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 |████████████████████████████████| 2137.6 MB 222 bytes/s Collecting torchvision==0.11.1+cu111 Downloading https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (24.5 MB) |████████████████████████████████| 24.5 MB 71.3 MB/s Collecting torchtext==0.11.0 Downloading torchtext-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB) |████████████████████████████████| 8.0 MB 25.4 MB/s Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0+cu111) (4.1.1) Requirement already satisfied: pillow!=8.3.0,&gt;=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1+cu111) (7.1.2) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1+cu111) (1.21.6) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.0) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (2022.6.15) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (3.0.4) Installing collected packages: torch, torchvision, torchtext Attempting uninstall: torch Found existing installation: torch 1.12.0+cu113 Uninstalling torch-1.12.0+cu113: Successfully uninstalled torch-1.12.0+cu113 Attempting uninstall: torchvision Found existing installation: torchvision 0.13.0+cu113 Uninstalling torchvision-0.13.0+cu113: Successfully uninstalled torchvision-0.13.0+cu113 Attempting uninstall: torchtext Found existing installation: torchtext 0.13.0 Uninstalling torchtext-0.13.0: Successfully uninstalled torchtext-0.13.0 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.10.0+cu111 which is incompatible. Successfully installed torch-1.10.0+cu111 torchtext-0.11.0 torchvision-0.11.1+cu111 - Installing mmcv |████████████████████████████████| 51.3 MB 136 kB/s |████████████████████████████████| 190 kB 29.4 MB/s - Installing mmdet |████████████████████████████████| 1.2 MB 32.1 MB/s - Installing mmseg |████████████████████████████████| 686 kB 26.6 MB/s - Installing icevision from master |████████████████████████████████| 3.1 MB 28.7 MB/s |████████████████████████████████| 55 kB 4.3 MB/s |████████████████████████████████| 98 kB 8.3 MB/s |████████████████████████████████| 49 kB 5.6 MB/s |████████████████████████████████| 111 kB 62.9 MB/s |████████████████████████████████| 97 kB 5.0 MB/s |████████████████████████████████| 798 kB 16.6 MB/s |████████████████████████████████| 58 kB 3.9 MB/s |████████████████████████████████| 188 kB 47.1 MB/s |████████████████████████████████| 700 kB 11.2 MB/s |████████████████████████████████| 1.8 MB 52.8 MB/s |████████████████████████████████| 79 kB 7.7 MB/s |████████████████████████████████| 509 kB 59.6 MB/s |████████████████████████████████| 596 kB 54.3 MB/s |████████████████████████████████| 117 kB 50.4 MB/s |████████████████████████████████| 141 kB 41.3 MB/s |████████████████████████████████| 5.8 MB 52.6 MB/s |████████████████████████████████| 419 kB 56.2 MB/s |████████████████████████████████| 50 kB 6.3 MB/s |████████████████████████████████| 42 kB 1.0 MB/s |████████████████████████████████| 154 kB 41.8 MB/s |████████████████████████████████| 87 kB 7.2 MB/s |████████████████████████████████| 97 kB 7.0 MB/s |████████████████████████████████| 181 kB 48.2 MB/s |████████████████████████████████| 157 kB 46.2 MB/s |████████████████████████████████| 63 kB 1.5 MB/s |████████████████████████████████| 157 kB 46.9 MB/s |████████████████████████████████| 156 kB 68.4 MB/s Building wheel for icevision (setup.py) ... done Building wheel for antlr4-python3-runtime (setup.py) ... done Building wheel for fire (setup.py) ... done Building wheel for fvcore (setup.py) ... done Building wheel for iopath (setup.py) ... done Building wheel for pathtools (setup.py) ... done ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow 2.8.2+zzzcolab20220719082949 requires tensorboard&lt;2.9,&gt;=2.8, but you have tensorboard 2.9.1 which is incompatible. flask 1.1.4 requires click&lt;8.0,&gt;=5.1, but you have click 8.0.4 which is incompatible. - Installing icedata from master Building wheel for icedata (setup.py) ... done Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting opencv-python-headless==4.1.2.30 Downloading opencv_python_headless-4.1.2.30-cp37-cp37m-manylinux1_x86_64.whl (21.8 MB) |████████████████████████████████| 21.8 MB 1.3 MB/s Requirement already satisfied: numpy&gt;=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.1.2.30) (1.21.6) Installing collected packages: opencv-python-headless Attempting uninstall: opencv-python-headless Found existing installation: opencv-python-headless 4.6.0.66 Uninstalling opencv-python-headless-4.6.0.66: Successfully uninstalled opencv-python-headless-4.6.0.66 Successfully installed opencv-python-headless-4.1.2.30 icevision installation finished! . . # We need to restart the kernel after installation import IPython IPython.Application.instance().kernel.do_shutdown(True) . Imports . We can import everything we need with just &#39;from icevision.all import *&#39;. . from icevision.all import * . INFO - Downloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf | icevision.visualize.utils:get_default_font:70 INFO - Downloading mmdet configs | icevision.models.mmdet.download_configs:download_mmdet_configs:31 . Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf... . INFO - Downloading mmseg configs | icevision.models.mmseg.download_configs:download_mmseg_configs:33 . Lets download a sample dataset from Icevision. . In the near future,we will work with our custom datasets for our custom needs. . url = &quot;https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip&quot; dest_dir = &quot;fridge&quot; data_dir = icedata.load_data(url, dest_dir) . Dataset Parsing . Icevision has very neat tricks for parsing both VOC and COCO type datasets.It can also identify our annotation errors and corrects them automatically.This capability is called &#39;autofix&#39;.Records is a very important concept in Icevision that holds information about image and its annotation.It is very extensible and can be customized for other object annotation types. . parser = parsers.VOCBBoxParser(annotations_dir=data_dir / &quot;odFridgeObjects/annotations&quot;, images_dir=data_dir / &quot;odFridgeObjects/images&quot;) . train_records, valid_records = parser.parse() parser.class_map . INFO - Autofixing records | icevision.parsers.parser:parse:122 . &lt;ClassMap: {&#39;background&#39;: 0, &#39;carton&#39;: 1, &#39;milk_bottle&#39;: 2, &#39;can&#39;: 3, &#39;water_bottle&#39;: 4}&gt; . Augmentations . &#39;Data augmentations are essential for robust training and results on many datasets and deep learning tasks. IceVision ships with the Albumentations library for defining and executing transformations, but can be extended to use others. For this tutorial, we apply the Albumentation&#39;s default aug_tfms to the training set. aug_tfms randomly applies broadly useful transformations including rotation, cropping, horizontal flips, and more. See the Albumentations documentation to learn how to customize each transformation more fully. The validation set is only resized (with padding). We then create Datasets for both. The dataset applies the transforms to the annotations (such as bounding boxes) and images in the data records.&#39; . # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=image_size, presize=512),tfms.A.Perspective(p=0.01),tfms.A.Normalize()]) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(image_size), tfms.A.Normalize()]) . train_ds = Dataset(train_records, train_tfms) valid_ds = Dataset(valid_records, valid_tfms) . Understanding the transforms . &#39;The Dataset transforms are only applied when we grab (get) an item. Several of the default aug_tfms have a random element to them. For example, one might perform a rotation with probability 0.5 where the angle of rotation is randomly selected between +45 and -45 degrees. This means that the learner sees a slightly different version of an image each time it is accessed. This effectively increases the size of the dataset and improves learning. We can look at result of getting the 0th image from the dataset a few times and see the differences. Each time you run the next cell, you will see different results due to the random element in applying transformations.&#39; You can add your custom albumentation transforms to train_tfms just as I did with &#39;tfms.A.Perspective&#39;. . samples = [train_ds[0] for _ in range(3)] show_samples(samples, ncols=3) . Selecting a Library and a Model . It is time to select our model ! We have a lot of options but I recommend you to try one of the mmdetection library models because they have a lot of state of the art implementations of models(Especially Vfnet). . Creating a model . &#39;Selections only take two simple lines of code. For example, to try the mmdet library using the retinanet model and the resnet50_fpn_1x backbone could be specified by:model_type = models.mmdet.retinanetbackbone = model_type.backbones.resnet50_fpn_1x(pretrained=True) As pretrained models are used by default, we typically leave this out of the backbone creation step.&#39; . selection = 0 extra_args = {} if selection == 0: model_type = models.mmdet.vfnet backbone = model_type.backbones.resnet50_fpn_mstrain_2x if selection == 1: model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x # extra_args[&#39;cfg_options&#39;] = { # &#39;model.bbox_head.loss_bbox.loss_weight&#39;: 2, # &#39;model.bbox_head.loss_cls.loss_weight&#39;: 0.8, # } if selection == 2: model_type = models.mmdet.faster_rcnn backbone = model_type.backbones.resnet101_fpn_2x # extra_args[&#39;cfg_options&#39;] = { # &#39;model.roi_head.bbox_head.loss_bbox.loss_weight&#39;: 2, # &#39;model.roi_head.bbox_head.loss_cls.loss_weight&#39;: 0.8, # } if selection == 3: model_type = models.mmdet.ssd backbone = model_type.backbones.ssd300 if selection == 4: model_type = models.mmdet.yolox backbone = model_type.backbones.yolox_s_8x8 if selection == 5: model_type = models.mmdet.yolof backbone = model_type.backbones.yolof_r50_c5_8x8_1x_coco if selection == 6: model_type = models.mmdet.detr backbone = model_type.backbones.r50_8x2_150e_coco if selection == 7: model_type = models.mmdet.deformable_detr backbone = model_type.backbones.twostage_refine_r50_16x2_50e_coco if selection == 8: model_type = models.mmdet.fsaf backbone = model_type.backbones.x101_64x4d_fpn_1x_coco if selection == 9: model_type = models.mmdet.sabl backbone = model_type.backbones.r101_fpn_gn_2x_ms_640_800_coco if selection == 10: model_type = models.mmdet.centripetalnet backbone = model_type.backbones.hourglass104_mstest_16x6_210e_coco elif selection == 11: # The Retinanet model is also implemented in the torchvision library model_type = models.torchvision.retinanet backbone = model_type.backbones.resnet50_fpn elif selection == 12: model_type = models.ross.efficientdet backbone = model_type.backbones.tf_lite0 # The efficientdet model requires an img_size parameter extra_args[&#39;img_size&#39;] = image_size elif selection == 13: model_type = models.ultralytics.yolov5 backbone = model_type.backbones.small # The yolov5 model requires an img_size parameter extra_args[&#39;img_size&#39;] = image_size model_type, backbone, extra_args . (&lt;module &#39;icevision.models.mmdet.models.vfnet&#39; from &#39;/usr/local/lib/python3.7/dist-packages/icevision/models/mmdet/models/vfnet/__init__.py&#39;&gt;, &lt;icevision.models.mmdet.models.vfnet.backbones.backbone_config.MMDetVFNETBackboneConfig at 0x7f42166a1c10&gt;, {}) . backbone.__dict__ . . {&#39;config_path&#39;: Path(&#39;/root/.icevision/mmdetection_configs/mmdetection_configs-2.20.1/configs/vfnet/vfnet_r50_fpn_mstrain_2x_coco.py&#39;), &#39;model_name&#39;: &#39;vfnet&#39;, &#39;pretrained&#39;: True, &#39;weights_url&#39;: &#39;https://download.openmmlab.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mstrain_2x_coco/vfnet_r50_fpn_mstrain_2x_coco_20201027-7cc75bd2.pth&#39;} . # Instantiate the model model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), **extra_args) . /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/builder.py:17: UserWarning: ``build_anchor_generator`` would be deprecated soon, please use ``build_prior_generator`` &#39;``build_anchor_generator`` would be deprecated soon, please use &#39; 2022-08-07 16:50:58,618 - mmcv - INFO - initialize ResNet with init_cfg {&#39;type&#39;: &#39;Pretrained&#39;, &#39;checkpoint&#39;: &#39;torchvision://resnet50&#39;} 2022-08-07 16:50:58,620 - mmcv - INFO - load model from: torchvision://resnet50 2022-08-07 16:50:58,621 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50 2022-08-07 16:50:58,731 - mmcv - WARNING - The model and loaded state dict do not match exactly unexpected key in source state_dict: fc.weight, fc.bias 2022-08-07 16:50:58,763 - mmcv - INFO - initialize FPN with init_cfg {&#39;type&#39;: &#39;Xavier&#39;, &#39;layer&#39;: &#39;Conv2d&#39;, &#39;distribution&#39;: &#39;uniform&#39;} 2022-08-07 16:50:58,795 - mmcv - INFO - initialize VFNetHead with init_cfg {&#39;type&#39;: &#39;Normal&#39;, &#39;layer&#39;: &#39;Conv2d&#39;, &#39;std&#39;: 0.01, &#39;override&#39;: {&#39;type&#39;: &#39;Normal&#39;, &#39;name&#39;: &#39;vfnet_cls&#39;, &#39;std&#39;: 0.01, &#39;bias_prob&#39;: 0.01}} 2022-08-07 16:50:58,833 - mmcv - INFO - backbone.conv1.weight - torch.Size([64, 3, 7, 7]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,835 - mmcv - INFO - backbone.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,839 - mmcv - INFO - backbone.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,842 - mmcv - INFO - backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,844 - mmcv - INFO - backbone.layer1.0.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,847 - mmcv - INFO - backbone.layer1.0.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,850 - mmcv - INFO - backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,852 - mmcv - INFO - backbone.layer1.0.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,855 - mmcv - INFO - backbone.layer1.0.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,858 - mmcv - INFO - backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,860 - mmcv - INFO - backbone.layer1.0.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,863 - mmcv - INFO - backbone.layer1.0.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,866 - mmcv - INFO - backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,868 - mmcv - INFO - backbone.layer1.0.downsample.1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,871 - mmcv - INFO - backbone.layer1.0.downsample.1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,874 - mmcv - INFO - backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,877 - mmcv - INFO - backbone.layer1.1.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,879 - mmcv - INFO - backbone.layer1.1.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,883 - mmcv - INFO - backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,886 - mmcv - INFO - backbone.layer1.1.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,888 - mmcv - INFO - backbone.layer1.1.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,890 - mmcv - INFO - backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,892 - mmcv - INFO - backbone.layer1.1.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,894 - mmcv - INFO - backbone.layer1.1.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,896 - mmcv - INFO - backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,898 - mmcv - INFO - backbone.layer1.2.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,901 - mmcv - INFO - backbone.layer1.2.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,902 - mmcv - INFO - backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,904 - mmcv - INFO - backbone.layer1.2.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,905 - mmcv - INFO - backbone.layer1.2.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,907 - mmcv - INFO - backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,908 - mmcv - INFO - backbone.layer1.2.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,909 - mmcv - INFO - backbone.layer1.2.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,911 - mmcv - INFO - backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,912 - mmcv - INFO - backbone.layer2.0.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,914 - mmcv - INFO - backbone.layer2.0.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,916 - mmcv - INFO - backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,918 - mmcv - INFO - backbone.layer2.0.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,920 - mmcv - INFO - backbone.layer2.0.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,922 - mmcv - INFO - backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,923 - mmcv - INFO - backbone.layer2.0.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,926 - mmcv - INFO - backbone.layer2.0.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,927 - mmcv - INFO - backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,930 - mmcv - INFO - backbone.layer2.0.downsample.1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,932 - mmcv - INFO - backbone.layer2.0.downsample.1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,934 - mmcv - INFO - backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,936 - mmcv - INFO - backbone.layer2.1.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,938 - mmcv - INFO - backbone.layer2.1.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,940 - mmcv - INFO - backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,942 - mmcv - INFO - backbone.layer2.1.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,944 - mmcv - INFO - backbone.layer2.1.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,946 - mmcv - INFO - backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,948 - mmcv - INFO - backbone.layer2.1.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,950 - mmcv - INFO - backbone.layer2.1.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,951 - mmcv - INFO - backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,953 - mmcv - INFO - backbone.layer2.2.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,955 - mmcv - INFO - backbone.layer2.2.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,957 - mmcv - INFO - backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,958 - mmcv - INFO - backbone.layer2.2.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,960 - mmcv - INFO - backbone.layer2.2.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,961 - mmcv - INFO - backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,962 - mmcv - INFO - backbone.layer2.2.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,964 - mmcv - INFO - backbone.layer2.2.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,965 - mmcv - INFO - backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,967 - mmcv - INFO - backbone.layer2.3.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,968 - mmcv - INFO - backbone.layer2.3.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,969 - mmcv - INFO - backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,971 - mmcv - INFO - backbone.layer2.3.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,972 - mmcv - INFO - backbone.layer2.3.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,974 - mmcv - INFO - backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,976 - mmcv - INFO - backbone.layer2.3.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,977 - mmcv - INFO - backbone.layer2.3.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,978 - mmcv - INFO - backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,979 - mmcv - INFO - backbone.layer3.0.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,980 - mmcv - INFO - backbone.layer3.0.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,982 - mmcv - INFO - backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,983 - mmcv - INFO - backbone.layer3.0.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,985 - mmcv - INFO - backbone.layer3.0.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,988 - mmcv - INFO - backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,989 - mmcv - INFO - backbone.layer3.0.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,991 - mmcv - INFO - backbone.layer3.0.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,992 - mmcv - INFO - backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,994 - mmcv - INFO - backbone.layer3.0.downsample.1.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,995 - mmcv - INFO - backbone.layer3.0.downsample.1.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,997 - mmcv - INFO - backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:58,998 - mmcv - INFO - backbone.layer3.1.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,000 - mmcv - INFO - backbone.layer3.1.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,001 - mmcv - INFO - backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,003 - mmcv - INFO - backbone.layer3.1.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,004 - mmcv - INFO - backbone.layer3.1.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,006 - mmcv - INFO - backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,007 - mmcv - INFO - backbone.layer3.1.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,009 - mmcv - INFO - backbone.layer3.1.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,010 - mmcv - INFO - backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,012 - mmcv - INFO - backbone.layer3.2.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,013 - mmcv - INFO - backbone.layer3.2.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,015 - mmcv - INFO - backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,016 - mmcv - INFO - backbone.layer3.2.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,018 - mmcv - INFO - backbone.layer3.2.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,019 - mmcv - INFO - backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,021 - mmcv - INFO - backbone.layer3.2.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,022 - mmcv - INFO - backbone.layer3.2.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,024 - mmcv - INFO - backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,026 - mmcv - INFO - backbone.layer3.3.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,027 - mmcv - INFO - backbone.layer3.3.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,029 - mmcv - INFO - backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,030 - mmcv - INFO - backbone.layer3.3.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,032 - mmcv - INFO - backbone.layer3.3.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,033 - mmcv - INFO - backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,035 - mmcv - INFO - backbone.layer3.3.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,036 - mmcv - INFO - backbone.layer3.3.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,038 - mmcv - INFO - backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,039 - mmcv - INFO - backbone.layer3.4.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,041 - mmcv - INFO - backbone.layer3.4.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,042 - mmcv - INFO - backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,044 - mmcv - INFO - backbone.layer3.4.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,045 - mmcv - INFO - backbone.layer3.4.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,047 - mmcv - INFO - backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,048 - mmcv - INFO - backbone.layer3.4.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,050 - mmcv - INFO - backbone.layer3.4.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,051 - mmcv - INFO - backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,053 - mmcv - INFO - backbone.layer3.5.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,055 - mmcv - INFO - backbone.layer3.5.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,056 - mmcv - INFO - backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,058 - mmcv - INFO - backbone.layer3.5.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,059 - mmcv - INFO - backbone.layer3.5.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,061 - mmcv - INFO - backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,062 - mmcv - INFO - backbone.layer3.5.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,064 - mmcv - INFO - backbone.layer3.5.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,065 - mmcv - INFO - backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,067 - mmcv - INFO - backbone.layer4.0.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,068 - mmcv - INFO - backbone.layer4.0.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,070 - mmcv - INFO - backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,071 - mmcv - INFO - backbone.layer4.0.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,073 - mmcv - INFO - backbone.layer4.0.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,074 - mmcv - INFO - backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,076 - mmcv - INFO - backbone.layer4.0.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,077 - mmcv - INFO - backbone.layer4.0.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,079 - mmcv - INFO - backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,080 - mmcv - INFO - backbone.layer4.0.downsample.1.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,082 - mmcv - INFO - backbone.layer4.0.downsample.1.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,083 - mmcv - INFO - backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,085 - mmcv - INFO - backbone.layer4.1.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,086 - mmcv - INFO - backbone.layer4.1.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,088 - mmcv - INFO - backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,089 - mmcv - INFO - backbone.layer4.1.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,091 - mmcv - INFO - backbone.layer4.1.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,093 - mmcv - INFO - backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,094 - mmcv - INFO - backbone.layer4.1.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,096 - mmcv - INFO - backbone.layer4.1.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,097 - mmcv - INFO - backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,099 - mmcv - INFO - backbone.layer4.2.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,100 - mmcv - INFO - backbone.layer4.2.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,102 - mmcv - INFO - backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,103 - mmcv - INFO - backbone.layer4.2.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,105 - mmcv - INFO - backbone.layer4.2.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,106 - mmcv - INFO - backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,108 - mmcv - INFO - backbone.layer4.2.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,110 - mmcv - INFO - backbone.layer4.2.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-07 16:50:59,112 - mmcv - INFO - neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-07 16:50:59,113 - mmcv - INFO - neck.lateral_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,115 - mmcv - INFO - neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-07 16:50:59,116 - mmcv - INFO - neck.lateral_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,118 - mmcv - INFO - neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-07 16:50:59,120 - mmcv - INFO - neck.lateral_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,121 - mmcv - INFO - neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-07 16:50:59,123 - mmcv - INFO - neck.fpn_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,124 - mmcv - INFO - neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-07 16:50:59,126 - mmcv - INFO - neck.fpn_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,127 - mmcv - INFO - neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-07 16:50:59,129 - mmcv - INFO - neck.fpn_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,130 - mmcv - INFO - neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-07 16:50:59,132 - mmcv - INFO - neck.fpn_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,133 - mmcv - INFO - neck.fpn_convs.4.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-07 16:50:59,135 - mmcv - INFO - neck.fpn_convs.4.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,136 - mmcv - INFO - bbox_head.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-07 16:50:59,138 - mmcv - INFO - bbox_head.cls_convs.0.gn.weight - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,140 - mmcv - INFO - bbox_head.cls_convs.0.gn.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,141 - mmcv - INFO - bbox_head.cls_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-07 16:50:59,143 - mmcv - INFO - bbox_head.cls_convs.1.gn.weight - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,144 - mmcv - INFO - bbox_head.cls_convs.1.gn.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,146 - mmcv - INFO - bbox_head.cls_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-07 16:50:59,147 - mmcv - INFO - bbox_head.cls_convs.2.gn.weight - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,149 - mmcv - INFO - bbox_head.cls_convs.2.gn.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,150 - mmcv - INFO - bbox_head.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-07 16:50:59,152 - mmcv - INFO - bbox_head.reg_convs.0.gn.weight - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,153 - mmcv - INFO - bbox_head.reg_convs.0.gn.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,155 - mmcv - INFO - bbox_head.reg_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-07 16:50:59,156 - mmcv - INFO - bbox_head.reg_convs.1.gn.weight - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,158 - mmcv - INFO - bbox_head.reg_convs.1.gn.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,159 - mmcv - INFO - bbox_head.reg_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-07 16:50:59,161 - mmcv - INFO - bbox_head.reg_convs.2.gn.weight - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,162 - mmcv - INFO - bbox_head.reg_convs.2.gn.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,164 - mmcv - INFO - bbox_head.vfnet_reg_conv.conv.weight - torch.Size([256, 256, 3, 3]): Initialized by user-defined `init_weights` in ConvModule 2022-08-07 16:50:59,166 - mmcv - INFO - bbox_head.vfnet_reg_conv.gn.weight - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,167 - mmcv - INFO - bbox_head.vfnet_reg_conv.gn.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,169 - mmcv - INFO - bbox_head.vfnet_reg.weight - torch.Size([4, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-07 16:50:59,170 - mmcv - INFO - bbox_head.vfnet_reg.bias - torch.Size([4]): NormalInit: mean=0, std=0.01, bias=0 2022-08-07 16:50:59,172 - mmcv - INFO - bbox_head.scales.0.scale - torch.Size([]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,173 - mmcv - INFO - bbox_head.scales.1.scale - torch.Size([]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,175 - mmcv - INFO - bbox_head.scales.2.scale - torch.Size([]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,176 - mmcv - INFO - bbox_head.scales.3.scale - torch.Size([]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,178 - mmcv - INFO - bbox_head.scales.4.scale - torch.Size([]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,180 - mmcv - INFO - bbox_head.vfnet_reg_refine_dconv.weight - torch.Size([256, 256, 3, 3]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,181 - mmcv - INFO - bbox_head.vfnet_reg_refine.weight - torch.Size([4, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-07 16:50:59,183 - mmcv - INFO - bbox_head.vfnet_reg_refine.bias - torch.Size([4]): NormalInit: mean=0, std=0.01, bias=0 2022-08-07 16:50:59,185 - mmcv - INFO - bbox_head.scales_refine.0.scale - torch.Size([]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,186 - mmcv - INFO - bbox_head.scales_refine.1.scale - torch.Size([]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,188 - mmcv - INFO - bbox_head.scales_refine.2.scale - torch.Size([]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,189 - mmcv - INFO - bbox_head.scales_refine.3.scale - torch.Size([]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,191 - mmcv - INFO - bbox_head.scales_refine.4.scale - torch.Size([]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,192 - mmcv - INFO - bbox_head.vfnet_cls_dconv.weight - torch.Size([256, 256, 3, 3]): The value is the same before and after calling `init_weights` of VFNet 2022-08-07 16:50:59,194 - mmcv - INFO - bbox_head.vfnet_cls.weight - torch.Size([4, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=-4.59511985013459 2022-08-07 16:50:59,195 - mmcv - INFO - bbox_head.vfnet_cls.bias - torch.Size([4]): NormalInit: mean=0, std=0.01, bias=-4.59511985013459 . load checkpoint from local path: checkpoints/vfnet/vfnet_r50_fpn_mstrain_2x_coco_20201027-7cc75bd2.pth The model and loaded state dict do not match exactly size mismatch for bbox_head.vfnet_cls.weight: copying a param with shape torch.Size([80, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([4, 256, 3, 3]). size mismatch for bbox_head.vfnet_cls.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([4]). . . Data Loader . &#39;The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl. Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training&#39; . train_dl = model_type.train_dl(train_ds, batch_size=8, num_workers=4, shuffle=True) valid_dl = model_type.valid_dl(valid_ds, batch_size=8, num_workers=4, shuffle=False) . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) . Let&#39;s visualize a batch of images. . model_type.show_batch(first(valid_dl), ncols=4) . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) . Metrics . &#39;The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning.&#39; . metrics = [COCOMetric(metric_type=COCOMetricType.bbox)] . Training . IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai, and pytorch-lightning.This makes it so convenient and easy to train models.This also means we can use specific engines&#39; callback systems. . Training using fastai . learn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics) . lr = learn.lr_find()[0] . /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn(&#39;``grid_anchors`` would be deprecated soon. &#39; /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` &#39;``single_level_grid_anchors`` would be deprecated soon. &#39; . lr . . 0.00019054606673307717 . learn.fine_tune(20, lr, freeze_epochs=1) . epoch train_loss valid_loss COCOMetric time . 0 | 3.595542 | 2.214065 | 0.419581 | 00:09 | . /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn(&#39;``grid_anchors`` would be deprecated soon. &#39; /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` &#39;``single_level_grid_anchors`` would be deprecated soon. &#39; . epoch train_loss valid_loss COCOMetric time . 0 | 2.084602 | 1.634030 | 0.470653 | 00:10 | . 1 | 1.858727 | 1.432253 | 0.444948 | 00:10 | . 2 | 1.676041 | 1.159493 | 0.714662 | 00:10 | . 3 | 1.521851 | 1.004197 | 0.796917 | 00:10 | . 4 | 1.398202 | 0.944683 | 0.839429 | 00:10 | . 5 | 1.288910 | 0.837564 | 0.897281 | 00:10 | . 6 | 1.186669 | 0.767936 | 0.898159 | 00:10 | . 7 | 1.098148 | 0.754201 | 0.922980 | 00:10 | . 8 | 1.030176 | 0.718864 | 0.897819 | 00:10 | . 9 | 0.975276 | 0.705527 | 0.907615 | 00:10 | . 10 | 0.925214 | 0.657741 | 0.933411 | 00:10 | . 11 | 0.877626 | 0.652339 | 0.924390 | 00:10 | . 12 | 0.849741 | 0.627645 | 0.933406 | 00:10 | . 13 | 0.819354 | 0.635671 | 0.912690 | 00:10 | . 14 | 0.794682 | 0.595221 | 0.925068 | 00:10 | . 15 | 0.769964 | 0.593487 | 0.931168 | 00:10 | . 16 | 0.744934 | 0.591581 | 0.927655 | 00:10 | . 17 | 0.724243 | 0.589162 | 0.927391 | 00:10 | . 18 | 0.707685 | 0.586102 | 0.930337 | 00:10 | . 19 | 0.686125 | 0.585268 | 0.930337 | 00:10 | . Training with pytorch lightning is extremely easy aswell. . Uncomment below lines to train with lightning. . # class LightModel(model_type.lightning.ModelAdapter): # def configure_optimizers(self): # return Adam(self.parameters(), lr=1e-4) # light_model = LightModel(model, metrics=metrics) # trainer = pl.Trainer(max_epochs=5, gpus=1) # trainer.fit(light_model, train_dl, valid_dl) . . Showing Results . &#39;The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function.&#39; . model_type.show_results(model, valid_ds, detection_threshold=0.5) . Prediction . &#39;Sometimes you want to have more control than show_results provides. You can construct an inference dataloader using infer_dl from any IceVision dataset and pass this to predict_dl and use show_preds to look at the predictions. . A prediction is returned as a dict with keys: scores, labels, bboxes, and possibly masks. . Prediction functions that take a detection_threshold argument will only return the predictions whose score is above the threshold. . Prediction functions that take a keep_images argument will only return the (tensor representation of the) image when it is True. In interactive environments, such as a notebook, it is helpful to see the image with bounding boxes and labels applied. In a deployment context, however, it is typically more useful (and efficient) to return the bounding boxes by themselves.&#39; . infer_dl = model_type.infer_dl(valid_ds, batch_size=4, shuffle=False) preds = model_type.predict_from_dl(model, infer_dl, keep_images=True) . show_preds(preds=preds[:4]) . Saving Our Model . For saving our model we need to import &#39;from icevision.models import *&#39;. . from icevision.models import * . checkpoint_path = &quot;/content/FridgeObjectsVfnet-ckpt.pth&quot; #Path to save our model with model name checkpoint_and_model = save_icevision_checkpoint(model, #our trained model filename = checkpoint_path, model_name=&#39;mmdet.vfnet&#39;, #our model name backbone_name=&#39;resnet50_fpn_mstrain_2x&#39;, #our model&#39;s backbone img_size=384, #our image size classes = parser.class_map.get_classes() #our classnames ) . Loading Our Model . checkpoint_path = &#39;/content/FridgeObjectsVfnet-ckpt.pth&#39; checkpoint_and_model = model_from_checkpoint(checkpoint_path) # Just logging the info model_type = checkpoint_and_model[&quot;model_type&quot;] backbone = checkpoint_and_model[&quot;backbone&quot;] class_map = checkpoint_and_model[&quot;class_map&quot;] #our classes img_size = checkpoint_and_model[&quot;img_size&quot;] #our image size model_type, backbone, class_map, img_size . model = checkpoint_and_model[&quot;model&quot;] # Transforms img_size = checkpoint_and_model[&quot;img_size&quot;] valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(img_size), tfms.A.Normalize()]) . Single Image Inference . !wget https://img.merkandi.com.tr/imgcache/800x600/offer/whatsapp-image-2022-04-18-at-100915-1650299189-1650299201.jpeg #lets grab an image from google . image = PIL.Image.open(&quot;/content/whatsapp-image-2022-04-18-at-100915-1650299189-1650299201.jpeg&quot;) . pred_dict = model_type.end2end_detect(image, valid_tfms, model, class_map=class_map, detection_threshold=0.5) #don&#39;t forget to play with detection_threshold ! pred_dict[&#39;img&#39;] . Batch Inference . infer_ds = Dataset.from_images([image for i in range(4)], valid_tfms, class_map=class_map) #our image repeated four times infer_dl = model_type.infer_dl(infer_ds , batch_size=4, shuffle=False) preds = model_type.predict_from_dl(model, infer_dl, keep_images=True) . show_preds(preds=preds[0:4]) . This notebook was inspired by; . https://airctic.com/dev/getting_started_object_detection/&gt; https://airctic.com/dev/inference/ .",
            "url": "https://uygarusta.github.io/UygarUsta/jupyter/2022/08/07/OD.html",
            "relUrl": "/jupyter/2022/08/07/OD.html",
            "date": " • Aug 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://uygarusta.github.io/UygarUsta/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://uygarusta.github.io/UygarUsta/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Uygar Usta , I am a Computer Vision Engineer and I aim to write blogposts about Computer Vision and Machine Learning. .",
          "url": "https://uygarusta.github.io/UygarUsta/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://uygarusta.github.io/UygarUsta/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}